<?xml version="1.0" encoding="UTF-8"?>
<faq>

    <group title="Overview">
        <entry><question>What's the purpose of this test framework?</question>
            <answer>
                <p>The test framework described here is intended primarily for
                unit and regression  tests.
                It can also be used for functional tests, and even simple
                product tests -- in other words, just about any type of test except a
                conformance test.  (Conformance tests belong in a Technology
                Compatibility Kit (TCK), such as the Java Compatibility Kit (JCK)).
                Tests can often be written as small standalone Java programs, although
                in some cases an applet or a shell-script might be required.</p>
            </answer>
        </entry>

        <entry><question>What's a regression test?</question>
            <answer>
                <p>A regression test is a test written specifically to check that a bug has
                been fixed and remains fixed.  A regression test should fail when run against a
                build with the bug in question, and pass when run against a build in which the
                bug has been fixed.</p>
            </answer>
        </entry>

        <entry><question>But I can also write non-regression tests using this framework?</question>
            <answer>
                <p>Yes.  Suppose, for example, that you evaluate a bug that turns out not
                to be a bug, but in the course of doing the evaluation you write a test that
                checks for correct behavior.  You can help improve the quality of the JDK by
                checking this test into the test directory.</p>
            </answer>
        </entry>

        <entry><question>What is the JavaTest<sup>TM</sup> harness?</question>
            <answer>
                <p>The JavaTest harness is a set of tools designed to execute test programs.
                It was originally designed to execute tests in the
                Java<sup>TM</sup> 2 SE Technology Compatibility Kit (TCK)
                (formerly the Java Compatibility Kit (JCK)).
                Among other things, the harness has evolved the ability to execute
                non-JCK testsuites.  The JDK regression test suite is one such suite.</p>

                <p>An open source version of the harness is available at
                <code>http://jtharness.java.net/</code>.
                </p>

            </answer>
        </entry>

        <entry>
            <question>What are the JDK regression extensions to the JavaTest harness?
                What is &quot;regtest&quot;?</question>
            <answer context="openjdk">
                <p>For the harness to execute tests in a given test suite, it needs
                specialized code which knows how to find test descriptions and how to interpret
                those descriptions.  The
                <a href="tag-spec.txt">JDK Test Framework: Tag Language Specification</a>
                provides the needed descriptions.
                <code>regtest</code> refers to extensions for the JavaTest harness that
                implement this specification, and is an older name for what is now known as
                "jtreg".</p>
            </answer>
            <answer context="help">
                <p>For the harness to execute tests in a given test suite, it needs
                specialized code which knows how to find test descriptions and how to interpret
                those descriptions.  The
                <a href="tag-spec.html">JDK Test Framework: Tag Language Specification</a>
                provides the needed descriptions.
                <code>regtest</code> refers to extensions for the JavaTest harness that
                implement this specification, and is an older name for what is now known as
                "jtreg".</p>
            </answer>
        </entry>

        <entry>
            <question>What are the system requirements
            for using the JDK regression extensions?</question>
            <answer>
                <p>It is recommended that you run jtreg on a platform that has been
                certified as Java Compatible.  It requires a version equivalent to
                JDK 1.5.0 or later.</p>

                <p>For Windows systems, the regression extensions require the
                installation of the <a href="http://www.mkssoftware.com/">MKS Toolkit</a>,
                version 6.1 or greater, or <a href="http://www.cygwin.com/">Cygwin</a>.</p>

            </answer>
        </entry>

        <entry>
            <question>Where can I find a copy of jtreg?</question>
            <answer context="openjdk">
                <p>You can find <b>jtreg</b> on the
                <a href="http://openjdk.java.net/jtreg">OpenJDK</a> web site.</p>
            </answer>
        </entry>

        <entry>
            <question>Where do I find additional supporting documentation?</question>
            <answer context="openjdk">
                <p>Beyond the Java<sup>TM</sup> Platform documentation, the following are
                relevant documentation resources.</p>

                <ul>
                    <li><a href="tag-spec.txt">JDK Test Framework: Tag
                    Language Specification</a> - The definitive document defining the test
                    description tags (syntax and behavior).</li>

                    <li>The <code>-help</code> option to <code>jtreg</code> offers brief
                    documentation for the complete set of currently available options.</li>
                    <li>
                    The JavaTest harness provides detailed online help via the
                    <code>Help</code> menu describing the graphical user interface. </li>
                </ul>

            </answer>
            <answer context="help">
                <p>Beyond the Java<sup>TM</sup> Platform documentation, the following are
                relevant documentation resources.</p>

                <ul>
                    <li><a href="tag-spec.html">JDK Test Framework: Tag
                    Language Specification</a> - The definitive document defining the test
                    description tags (syntax and behavior).</li>

                    <li>The <code>-help</code> option to <code>jtreg</code> offers brief
                    documentation for the complete set of currently available options.</li>
                    <li>
                    The JavaTest harness provides detailed online help via the
                    <code>Help</code> menu describing the graphical user interface. </li>
                </ul>

            </answer>
        </entry>

        <entry>
            <question>There's functionality missing from
            the tag specification. I can't write my test or it would vastly improve the
            life of people writing tests if it was added. What do I need to
            do?</question>
            <answer context="openjdk">
                <p>See the <a href="http://openjdk.dev.java.net/jtreg">OpenJDK</a> page
                    for a suitable forum or mailing list.</p>
            </answer>
        </entry>

        <entry>
            <question>The spec is fine, but there's some
            functionality that I'd like to get from the regression extensions or I
            still can't run it on a basic test. Who do I contact?</question>
            <answer context="sun">
                <p>Send email to
                <a href="mailto:jtreg-comments@sun.com">jtreg-comments@sun.com</a>.</p>
            </answer>
            <answer context="openjdk">
                <p>Send email to
                <a href="mailto:jtreg-discuss@openjdk.java.net">jtreg-discuss@openjdk.java.net</a>.</p>
            </answer>
        </entry>

        <entry>
            <question>Why not use JUnit?</question>
            <answer>
                <p>JUnit was not around when we started writing tests for JDK.
                And, the test tag specification has been specifically designed for testing
                JDK, with support for testing applets, command-line interfaces,
                and so on, as well as simple API tests.</p>

                <p>And by now, there are many thousands of tests written for jtreg,
                so it would not be practical to convert to JUnit.</p>
            </answer>
        </entry>

    </group>
    <group title="Writing a JDK Regression Test">

        <entry>
            <question>How do I write a test?</question>
            <answer>
                <p> The simplest test is an ordinary Java program with the usual static
                main method.  If the test fails, it should throw an exception; if it succeeds,
                it should return normally.</p>

                <p>Here's an example:</p>

                <dl><dt></dt><dd>
                    <pre>
/* @test 1.1 97/10/12
   @bug 1234567
   @summary Make sure that 1 != 0
*/

public class OneZero {

    public static void main(String[] args) throws Exception {
        if (1 == 0) {
            throw new Exception("1 == 0");
        }
    }

}
                    </pre>
                </dd></dl>

                <p>This test could be run on its own with the command</p>

                <dl><dt></dt><dd>
                <pre>&lt;build&gt;/bin/java OneZero</pre>
                </dd></dl>
                where &lt;build&gt; is the location of the JDK build that you're testing.

            </answer>
        </entry>

        <entry>

            <question>What does the <code>@test</code> tag mean?</question>
            <answer>
                <p>The <code>@test</code> tag identifies a source file that defines a test.
                the harness will automatically run any <code>.java</code>, <code>.sh</code>, and
                <code>.html</code> file containing an <code>@test</code> tag within the
                appropriate comment; it ignores any file not containing such a tag or not
                utilizing one of the expected extensions.</p>

                <p>If necessary the harness will compile the source file, if the class files are
                older than the corresponding source files.  Other files which the test depends
                on must be specified with the <code>@run build</code> action.</p>

                <p>The arguments to the <code>@test</code> tag are ignored by the harness.  For
                identification it may be useful to put information such as SCCS ID keywords after the <code>@test</code> tag.</p>

                <p>While not part of the tag specification, some tests use the
                string "<code>/nodynamiccopyright</code>" after <code>@test</code>
                to indicate that that the file should not be subject to automated
                copyright processing that might affect the operation of the test,
                for example, by affecting the line numbers of the test source code.</p>
            </answer>
        </entry>

        <entry>
            <question>What do the other tags mean?</question>
            <answer>
                <p>The other tags shown above are optional.</p>

                <p>The <code>@bug</code> tag should be followed by one or more bug numbers,
                separated by spaces.  The bug number is useful in diagnosing test failures.
                It's OK to write tests that don't have bug numbers, but if you're writing a
                test for a specific bug please include its number in an <code>@bug</code> tag.</p>

                <p>The <code>@summary</code> tag describes the condition that is checked by the
                test.  It is especially useful for non-regression tests, which by definition
                don't have bug numbers, but even if there's a bug number it's helpful to
                include a summary.  Note that a test summary is generally not the same thing as
                a Bugtraq synopsis, since the latter describes the bug rather than the
                condition that the bug violates.</p>
            </answer>
        </entry>

        <entry>
            <question>How are tag arguments delimited?</question>
            <answer>
                <p>The arguments of a tag are the words between that tag and the next tag,
                if there is one, or the end of the comment enclosing the tags.</p>
            </answer>
        </entry>

        <entry>
            <question>If a test fails, do I have to throw
            my own exception?</question>
            <answer>
                <p>No.  In cases like the above example in which you must check a condition
                in order to decide whether the test is to pass or fail, you have no choice but
                to construct an exception.  <code>RuntimeException</code> is a convenient
                choice since it's unchecked, so you don't have to sprinkle your code with
                throws clauses.</p>

                <p>On the other hand, if the test would naturally cause an exception to be
                thrown when it fails, it suffices to let that exception be propagated up
                through the main method.  If the exception you expect to be thrown in the
                failure case is a checked exception, then you'll need to provide the
                appropriate throws clauses in your code.</p>

                <p>In general, the advantage of throwing your own exception is that often you
                can provide better diagnostics.</p>

                <p>It is <em>strongly</em> recommended that you not catch general exceptions
                such as <code>Throwable</code>, <code>Exception</code>, or <code>Error</code>.
                Doing so can be potentially <a href="#question4.11">problematic</a>.</p>
            </answer>
        </entry>

        <entry>
            <question>Should a test call the <code>System.exit</code> method?</question>
            <answer>
                <p>No. Depending on how you run the tests, you may get a security exception
                from the harness.</p>
            </answer>
        </entry>

        <entry>
            <question>Can a test write to
            <code>System.out</code> and <code>System.err</code>?</question>
            <answer>
                <p>Yes. The harness will capture everything sent to both streams. </p>
            </answer>
        </entry>

        <entry>
            <question>Can a test check the output of
            <code>System.out</code> and <code>System.err</code>?</question>
            <answer>
                <p>Yes, compiler tests using the <code>@compile</code> tag can use
                the <code>/ref=<i>file</i></code> option.
                Such tests are generally not recommended, since the output can be
                sensitive to the locale in which the are run, and may contain
                other details which may be hard to maintain, such as line numbers.</p>
                <p>While not part of the tag specification, some tests use the
                string "<code>/nodynamiccopyright/</code>" after the <code>@test</code> tag
                to indicate that that the file should not be subject to automated
                copyright processing that might affect the operation of the test.</p>
            </answer>
        </entry>

        <entry>
            <question>What should I do with a test once
            I've written it?</question>
            <answer>
                <p>Check it into the test directory in your repository.</p>

                <p>Yes, it really is that simple.</p>
            </answer>
        </entry>

        <entry>
            <question>Where is the test directory?</question>
            <answer>
                <p>Within an OpenJDK forest, the langtools/ and jdk/ repositories
                each have a test/ directory.  The directory and its contents
                will be created automatically when you clone either of those
                repositories.</p>

                <p>The jdk/test directory contains tests for the main JRE API
                and related tools. The langtools/test directory contains tests
                for the javac, javadoc, javah and javap tools.</p>
            </answer>
        </entry>

        <entry>
            <question>Why not have separate test
            workspaces that only contain tests?</question>
            <answer>
                <p>Checking a test into the workspace against which it's first written
                helps prevent spurious test failures.  In the case of a regression test, this
                process ensures that a regression test will migrate upward in the integration
                hierarchy along with the fix for the corresponding bug.  If tests were managed
                separately from fixes then it would be difficult to distinguish between a true
                test failure and a failure due to version skew because the fix hadn't caught up
                with the test.</p>
            </answer>
        </entry>

        <entry>
            <question>How are the test directories organized?</question>
            <answer>
                <p>Tests are generally organized following the structure of the Java API.  For
                example, the <code>test</code> directory contains a <code>java</code> directory
                that has subdirectories <code>lang</code>, <code>io</code>, <code>util</code>,
                etc.</p>

                <p>Each package directory contains one subdirectory for each class in the
                package.  Thus tests for <code>java.io.FileInputStream</code> can be found in
                <code>java/io/FileInputStream</code>.</p>

                <p>Each class directory may contain a combination of single-file tests and
                further subdirectories for tests that require more than one source file.</p>
            </answer>
        </entry>

        <entry>
            <question>How should I name a test?</question>
            <answer>
                <p>In general, try to give tests names that are as specific and descriptive
                as possible.  If a test is checking the behavior of one or a few methods, its
                name should include the names of those methods.  For example, a test written
                for a bug in the skip method of FileInputStream could be placed in
                <code>test/java/io/FileInputStream/Skip.java</code>.  A test written for a bug
                that involves both the skip and available methods could be named
                <code>SkipAvailable.java</code>.</p>

                <p>Tests that involve many methods require a little more creativity in naming,
                since it would be unwieldy to include the names of all the methods.  Just
                choose a descriptive word or short phrase.  For example, a test that checks the
                general behavior of a FileInputStream after end-of-file has been reached could
                be named <code>AfterEOF.java</code>.</p>

                <p>It can be helpful to add more information to the test name to help further
                describe the test.  For example, a test that checks the skip method's behavior
                when passed a negative count could be named <code>SkipNegative.java</code>.</p>

                <p>You might find that the name you want to give your test has already been
                taken.  In this case either find a different name or, if you're just not in a
                creative mood, append an underscore and a digit to an existing name.  Thus if
                there were already a <code>Skip.java</code> file, a new test for the skip
                method could be named <code>Skip_1.java</code>.</p>

                <p>Some tests require more than one source file, or may need access to data
                files.  In this case it's best to create a subdirectory in order to keep
                related files together.  The subdirectory should be given a descriptive
                mixed-case name that begins with a lowercase letter.  For example, a
                FileInputStream test that needs an input file as well as its Java source file
                in order to test the interaction of the read and available methods could be
                placed in the subdirectory
                <code>test/java/io/FileInputStream/readAvailable</code>.</p>

                <p>Some tests involve more than one class in a package, in which case a new
                subdirectory in the relevant package directory should be created.  For example,
                a set of general tests that exercise the character streams in the
                <code>java.io</code> package could be placed in the
                <code>test/java/io/charStreams</code> directory.</p>
            </answer>
        </entry>

        <entry>
            <question>What about tests that don't fit
            into the API structure?</question>
            <answer>
                <p>In addition to a <code>java</code> directory for API-related tests, the
                test directory contains a <code>javax</code> directory,
                <code>vm</code> directory, a <code>tools</code>
                directory, and <code>com</code> and <code>sun</code> directories.</p>
            </answer>
        </entry>

        <entry>
            <question>Can tests in different directories
            have the same name?</question>
            <answer>
                <p>Yes.  When a test is run by the harness, a special classloader is used so
                that the classpath is effectively set to include just the directory containing
                the test, plus the standard system classes.  Thus name
                clashes between tests in different directories are not a problem.</p>

                <p>An alternative approach would be to associate a different package with each
                test directory.  This is done, for example, in the JCK test suite.  The
                difficulty with this idea is that in order to debug a test (under dbx or
                workshop or jdb or whatever) you must set up your classpath in just the right
                way.  This makes it difficult to diagnose bugs that are reported against
                specific tests.  </p>
            </answer>
        </entry>

        <entry>
            <question>How do I write a test for an AWT
            bug or a Swing bug?</question>
            <answer>
                <p>Bugs in the graphical facilities of the JDK generally require
                manual interaction with applets.  Applet tests are written in much the
                same way as the simple <code>main</code> tests described above.  The
                primary differences are that a second "@" tag is given to indicate
                that the test is an applet test, and an appropriate HTML file is
                needed.  For example, an AWT test named <code>Foo.java</code> would
                have the form:</p>

                <dl><dt></dt><dd>
                    <pre>
                        /* @test 1.1 97/10/12
                           @bug 9876543
                           @run applet/manual Foo.html */

                        public class Foo extends java.awt.Applet { ... }
                    </pre>
                </dd></dl>
                <p>or</p>
                <dl><dt></dt><dd>
                    <pre>
                        public class Foo extends javax.swing.JApplet { ... }
                    </pre>
                </dd></dl>

                <p>The <code>@run</code> tag tells the harness how to run the test.  The first
                argument is the run type, <code>applet</code>, followed by an option,
                <code>/manual</code>, that flags this test as a manual test requiring user
                interaction.  The remaining arguments to the <code>@run</code> tag are passed
                to the program in a manner appropriate to the run type.  In this case, the test
                will be run just as if the <code>appletviewer</code> had been invoked on
                <code>Foo.html</code>.  Thus <code>Foo.html</code> must contain, at least, an
                HTML <code>applet</code> tag with any necessary parameters.</p>
            </answer>
        </entry>

        <entry>
            <question>How does the user know what to do
            for a manual applet test?</question>
            <answer>
                <p>When the harness runs a manual applet test, it will display the contents of
                the HTML file that defines the applet.  Include instructions in the HTML file
                so that the person running the test can figure out what to do if any
                interaction is required.</p>
            </answer>
        </entry>

        <entry>
            <question>Exactly what does the
            <code>/manual</code> option mean?</question>
            <answer>
                <p>The <code>/manual</code> option indicates to the harness that this is a
                manual test.  This allows the harness to distinguish manual from automatic
                tests, which is important since the latter can be run without user interaction.</p>

                <p>There are actually three kinds of applet manual tests: Self-contained tests,
                <code>yesno</code> tests, and <code>done</code> tests.</p>

                <p>A self-contained manual test handles all user interaction itself.  If the
                test fails, whether this is determined by the user or by the applet, then the
                applet must throw an exception.  Self-contained tests specify
                <code>applet/manual</code> for the first <code>@run</code> argument.</p>

                <p>A <code>yesno</code> test requests the harness to ask the user whether the test
                passes or fails.  To do this, the harness will put up <code>pass</code> and
                <code>fail</code> buttons, and it's up to the user to inspect the screen and
                click one of the buttons.  The harness will take care of shutting down the applet.
                The test will also fail if the applet throws an exception.  <code>Yesno</code>
                tests specify <code>applet/manual=yesno</code> for the first <code>@run</code>
                argument.</p>

                <p>A <code>done</code> test requests the harness to put up a <code>done</code>
                button.  After the user has completed whatever actions are required by the
                test, the user clicks <code>done</code> and the harness shuts down the applet.
                The program must itself determine whether the test is to pass or fail, and
                throw an exception in the latter case.  <code>Done</code> tests specify
                <code>applet/manual=done</code> for the first <code>@run</code> argument.</p>

                <p><code>main</code> and <code>shell</code> may also specify the
                <code>manual</code> option using <code>main/manual</code> and
                <code>shell/manual</code> respectively.  These tests must be completely
                self-contained.</p>
            </answer>
        </entry>

        <entry>
            <question>How does a manual applet test
            indicate success or failure?</question>
            <answer>
                <p>Just as with <code>main</code> tests, an applet may throw an exception
                at any time to indicate test failure.  A <code>done</code> test applet, for
                example, can check for failure in the <code>java.applet.Applet.destroy</code>
                method.  If an applet doesn't throw an exception, and if, in the case of a
                <code>yesno</code> test, the user clicks <code>pass</code>, then the test is
                considered to have passed.</p>

                <p>Be very careful about where failure is checked.  The AWT event thread does
                not propagate exceptions!</p>
            </answer>
        </entry>
    </group>
    <group title="Getting Started">
        <entry>
            <question>What does a typical invocation of
            <code>jtreg</code> look like?  How can I make sure that I can even run
            the JavaTest harness?</question>
            <answer>
                <p>You may verify that the JavaTest harness  can be properly invoked by using
                <code>jtreg</code> to run this sample test.</p>

                <dl><dt></dt><dd>
                <pre>
                    /*
                     * @test
                     * @bug 2718282
                     * @summary Hello test
                     */

                    public class Hello {
                        public static void main(String [] args) throws Exception {
                            if (true)
                                System.out.println("Hello World!");
                            else
                                throw new Exception("??");
                        }
                    }
                </pre></dd></dl>

                <p>A typical invocation of <code>jtreg</code> on that test is:</p>

                <dl><dt></dt><dd>
                    <pre>
                        ribbit$ jtreg -verbose:all -testjdk:/usr/local/java/jdk1.4/solsparc Hello.java
                    </pre>
                </dd></dl>

                <p>where</p>
                <ul>
                    <li><code>-verbose:all</code> is a verbose option which causes output
                    from all tests (regardless of whether they passed or failed) to be
                    printed at completion of the test.</li>
                    <li><code>-testjdk</code> specifies the location of the JDK version which
                    should be used to run the test.</li>
                </ul>

                <p>Modulo the line numbers, output for the successful invocation of
                <code>jtreg</code> will look like:</p>

                <dl><dt></dt><dd>
                    <pre>
                         1   --------------------------------------------------
                         2   TEST: Hello.java
                         3   JDK under test: (/usr/local/java/jdk1.4/solsparc/jre)
                         4   java version "1.4.0-beta"
                         5   Java(TM) 2 Runtime Environment, Standard Edition (build 1.4.0-beta-b56)
                         6   Java HotSpot(TM) Client VM (build 1.4-beta-B56, mixed mode)
                         7
                         8   ACTION: build -- Passed. Compilation successful
                         9   REASON: Named class compiled on demand
                        10   TIME:   3.024 seconds
                        11
                        12   ACTION: compile -- Passed. Compilation successful
                        13   REASON: .class file out of date or does not exist
                        14   TIME:   3.023 seconds
                        15   STDOUT:
                        16   STDERR:
                        17
                        18   ACTION: main -- Passed. Execution successful
                        19   REASON: Assumed action based on file name: run main Hello
                        20   TIME:   0.862 seconds
                        21   STDOUT:
                        22   Hello World!
                        23   STDERR:
                        24   STATUS:Passed.
                        25
                        26   TEST RESULT: Passed. Execution successful
                        27   --------------------------------------------------
                        28   test results: passed: 1
                        29   Report written to /u/iag/jtwJTreport/report.html
                        30   Results written to /u/iag/jtw/JTwork
                    </pre>
                </dd></dl>

                <p>The test was compiled and executed. No exception was thrown during
                execution, thus the test passed.</p>

                <p>Interpretation of this output is as follows:</p>

                <ul>
                    <li>line 2 - The name of the test that was run.</li>
                    <li>line 3 - The JDK under test (should be identical to the value passed via
                    the <code>-testjdk</code> option).</li>
                    <li>line 4-6 - The product version produced when <code>java
                    [-JVMOptions]</code> version" is called for the JDK under test.  Valid
                    <code>[-JVMOptions]</code> include <code>-client</code>, <code>-server</code>,
                    <code>-hotspot</code>, <code>-d64</code>, and <code>-d32</code>, as
                    applicable to the current platform and test JDK.</li>
                    <li>lines 8-10, 12-16, 18-24 - The set of actions that were run according to
                    the test description provided. Each action contains five parts.</li>

                    <ul>
                        <li>the name of the action and its final status</li>
                        <li>the reason the action was taken</li>
                        <li>the amount of time to run the test</li>
                        <li>standard output (note line 22 of the <code>main</code> action contains the
                        string "Hello World!")</li>
                        <li>standard error</li>
                    </ul>

                    <li>line 26 - The final result of the test.</li>
                    <li>lines 28-30 - Summary information about all the tests that were run.</li>

                    <ul>
                        <li>line 28 - One test passed. This line would also indicate the number of
                        tests that failed, or that produced errors, as applicable.</li>
                        <li>line 29 - Location for <code>.html</code> reports.</li>
                        <li>line 30 - Location for auxiliary files generated during the test
                        execution. Of particular note are the results files (<code>.jtr</code>)
                        which contain information about the individual tests that were run.</li>
                    </ul>
                </ul>

            </answer>
        </entry>

        <entry>
            <question>Bleah! That verbose output is so
            long!  Can I have something shorter?</question>
            <answer>
                <p>Yes. Several different options provided with <code>jtreg</code>
                influence the output per test.  Here are a few verbose settings in order of
                decreasing average output per test.</p>

                <ul>
                    <li><a href="#V.0"><code>-verbose:fail</code></a> (and related
                    <code>-verbose:pass</code>, <code>-verbose:error</code>, and
                    <code>-verbose:all</code>)</li>
                    <li><a href="#V.1"><code>-verbose</code></a></li>
                    <li><a href="#V.2"><code>-verbose:summary</code></a></li>
                    <li><a href="#V.3">no verbose option</a></li>
                </ul>

                <p>The following samples of output correspond to each of the above settings.
                Each sample is run with three tests: <code>Pass.java</code>,
                <code>Fail.java</code>, and <code>Error.java</code> .  Note that in
                some cases, the output varies substantially depending on whether the test
                passed or failed.</p>

                <p><a name="V.0"><b><code>-verbose:fail</code></b></a> - Full output for failed
                tests only.  Two lines for tests that passed or produced errors (related
                options: <code>-verbose:pass</code>, <code>-verbose:fail</code>, and
                <code>-verbose:all</code>).</p>

                <dl><dt></dt><dd>
                    <pre>
                        ribbit$ jtreg -verbose:fail Pass.java Fail.java Error.java
                        --------------------------------------------------
                        TEST: Pass.java
                        TEST RESULT: Passed. Execution successful
                        --------------------------------------------------
                        TEST: Fail.java
                        JDK under test: (/usr/local/java/jdk1.4/solsparc)
                        java version "1.4.0-beta"
                        Java(TM) 2 Runtime Environment, Standard Edition (build 1.4.0-beta-b56)
                        Java HotSpot(TM) Client VM (build 1.4-beta-B56, mixed mode)

                        ACTION: build -- Passed. Compilation successful
                        REASON: Named class compiled on demand
                        TIME:   3.649 seconds

                        ACTION: compile -- Passed. Compilation successful
                        REASON: .class file out of date or does not exist
                        TIME:   3.637 seconds
                        STDOUT:
                        STDERR:

                        ACTION: main -- Failed. Execution failed: `main' threw exception: java.lang.Exception: Fail
                        REASON: Assumed action based on file name: run main Fail
                        TIME:   1.219 seconds
                        STDOUT:
                        STDERR:
                        java.lang.Exception: I failed
                         at Fail.main(Fail.java:5)
                         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
                         at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:30)
                         at sun.reflect.InflatableMethodAccessorImpl.invoke(InflatableMethodAccessorImpl.java:46)
                         at java.lang.reflect.Method.invoke(Method.java:306)
                         at com.sun.javatest.regtest.MainWrapper$MainThread.run(MainWrapper.java:94)
                         at java.lang.Thread.run(Thread.java:579)

                        JavaTest Message: Test threw exception: java.lang.Exception: I failed
                        JavaTest Message: shutting down test

                        STATUS:Failed.`main' threw exception: java.lang.Exception: I failed

                        TEST RESULT: Failed. Execution failed: `main' threw exception: java.lang.Exception: I failed
                        --------------------------------------------------
                        TEST: Error.java
                        TEST RESULT: Error. Parse Exception: Unexpected length for bugid: 31415926,
                        --------------------------------------------------
                        test results: passed: 1; failed: 1; error: 1
                        Report written to /u/iag/jtw/JTreport/report.html
                        Results written to /u/iag/jtw/JTwork
                        Error: some tests failed or other problems occurred
                    </pre>
                </dd></dl>

                <p><a name="V.1"><b><code>-verbose</code></b></a> - This option produces three
                lines of output per test: start, end, final status.</p>

                <dl><dt></dt><dd>
                    <pre>
                        ribbit$ jtreg -verbose Pass.java Fail.java Error.java
                        runner starting test: Pass.java
                        runner finished test: Pass.java
                        Passed. Execution successful
                        runner starting test: Fail.java
                        runner finished test: Fail.java
                        Failed. Execution failed: `main' threw exception: java.lang.Exception: I failed
                        runner starting test: Error.java
                        runner finished test: Error.java
                        Error. Parse Exception: Unexpected length for bugid: 31415926,
                        test results: passed: 1; failed: 1; error: 1
                        Report written to /u/iag/jtw/JTreport/report.html
                        Results written to /u/iag/jtw/JTwork
                        Error: some tests failed or other problems occurred
                    </pre>
                </dd></dl>

                <p><a name="V.2"><b><code>-verbose:summary</code></b></a> - A single line of
                output per test: final status and name of file.</p>

                <dl><dt></dt><dd>
                    <pre>
                        ribbit$ jtreg -verbose:summary Pass.java Fail.java Error.java
                        Passed: Pass.java
                        FAILED: Fail.java
                        Error:  Error.java
                        test results: passed: 1; failed: 1; error: 1
                        Report written to /u/iag/jtw/JTreport/report.html
                        Results written to /u/iag/jtw/JTwork
                        Error: some tests failed or other problems occurred
                    </pre>
                </dd></dl>

                <p><a name="V.3"><b>No verbose option</b></a> provides only general summary
                information about all the tests run.</p>

                <dl><dt></dt><dd>
                    <pre>
                        ribbit$ jtreg Pass.java Fail.java Error.java
                        test results: passed: 1; failed: 1; error: 1
                        Report written to /u/iag/jtw/JTreport/report.html
                        Results written to /u/iag/jtw/JTwork
                        Error: some tests failed or other problems occurred
                    </pre>
                </dd></dl>

                <p>If there is information that you find lacking in all of these options,
                please contact the developer to determine if it is possible to make it
                available.</p>
            </answer>
        </entry>

        <entry>
            <question>Are there abbreviations for these
            long options?</question>
            <answer>
                <p>Yes.  The <code>-help</code> option to <code>jtreg</code> lists the long
                and abbreviated forms of all options.</p>
            </answer>
        </entry>

        <entry>
            <question>How do I view what a test sends to
            <code>System.out/System.err</code>?</question>
            <answer>
                <p>You have several alternatives.</p>
                <ol>
                    <li>Use the <code>-verbose:all</code> option (or its result-sensitive related
                    options <code>-verbose:pass</code>, <code>-verbose:fail</code>,
                    <code>-verbose:error</code>) to <code>jtreg</code>.</li>
                    <li>Use the JavaTest harness GUI.</li>
                    <li>View the test's <code>.jtr</code> file.</li>
                </ol>
            </answer>
        </entry>

        <entry>
            <question>What is a <code>.jtr</code>
            file?</question>
            <answer>
                <p>As each test is run, it produces a JavaTest Results (<code>.jtr</code>)
                file which contains information about how the test was run, the name of the
                test, standard output, standard input, final status, etc.  The name of the file
                is the basename of the file containing the test description followed by the
                <code>.jtr</code> extension.  For example, <code>Hello.java</code> produces a
                file called <code>Hello.jtr</code>.  These files reside in the
                work directory which contains a directory hierarchy that
                parallels the source structure.</p>
            </answer>
        </entry>

        <entry>
            <question>What are the report and work directories?</question>
            <answer>
                <p>The report directory contains all final reports in HTML format.  By
                default, the name of the report directory is <code>JTreport</code>.  The name
                may be selected via the <code>-reportDir</code> option to <code>jtreg</code>.</p>

                <p>The work directory contains any files which are generated during a test run.
                These include <code>.class</code> and <code>.jtr</code>
                files.  By default, the name of the work directory is <code>JTwork</code>.  The
                name may be selected via the <code>-workDir</code> option to
                <code>jtreg</code>.</p>
            </answer>
        </entry>

        <entry>
            <question>What's the difference between the
            "fail" and "error" return status?</question>
            <answer>
                <p>If a test <em>failed</em>, then the harness was able to actually run the
                test code.  The failure could either indicate that the test truly failed
                (i.e. it threw an exception) or that there was some problem running the test
                due to security restrictions, lack of resources, etc.</p>

                <p>If <em>error</em> is the final result of a test, then the harness was unable to
                run the test.  An error is most commonly associated with problems in the test
                description (typos, missing required arguments, etc.).</p>

                <p>In either case the result contains a short message intended to provide hints
                    as to where the problem lies.
                </p>
            </answer>
        </entry>

        <entry>
            <question>If a test fails, I'd like to put
            all of my debugging information into the final result.  How do I do
            that?</question>
            <answer>
                <p>The final result string is composed by the harness.  For tests that fail
                because an exception is thrown, the result string will contain some header
                string such as <code>`main' threw exception: </code> followed by the
                exception's type and detail message.  This detail message should contain
                sufficient information to provide the test user a starting point to investigate
                the unexpected failure.  It should <em>not</em> contain full debugging
                information.</p>

                <p>The harness makes no guarantees as to the availability of any detail message
                longer than 128 characters.</p>
            </answer>
        </entry>

        <entry>
            <question>I've heard that the
            <code>jtreg</code> has a GUI. How do I access that?</question>
            <answer>
                <p>The complete JavaTest harness GUI is available via the <code>-gui</code> option
                to <code>jtreg</code>. The Online Help, available on the <code>Help</code>
                menu, provides a detailed description of the graphical user interface.
                You can also use the <code>F1</code> key to get context sensitive help
                about any selected item in the graphical user interface.
                </p>
            </answer>
        </entry>

        <entry>
            <question>Can I verify the correctness of
            test descriptions without actually running the tests?</question>
            <answer>
                <p>Yes!  The <code>-check</code> option to <code>jtreg</code> will find
                test descriptions and will determine whether they are written according to the
                <a href="http://javaweb.sfbay/jdk/test/tag-spec">tag specification</a>. Tests
                will <em>not</em> be executed.</p>

                <p>The following sample output illustrates use of this option.</p>

                <dl><dt></dt><dd>
                    <pre>
                        ribbit$ jtreg -verbose:summary -check Pass.java Fail.java Error.java
                        Passed: Pass.java
                        Passed: Fail.java
                        Error:  Error.java
                        test results: passed: 2; error: 1
                        Report written to /u/iag/jtw/JTreport/report.html
                        Results written to /u/iag/jtw/JTwork
                        Error: some tests failed or other problems occurred
                    </pre>
                </dd></dl>
            </answer>
        </entry>

        <entry>
            <question>Can I generate reports for tests
            that have already been run?</question>
            <answer>
                <p>Yes!  The <code>-reportOnly</code> option to <code>jtreg</code> will
                generate the standard HTML reports for tests which have been previously
                executed.  Tests will <em>not</em> be executed.  A <a href="#question3.6">work
                directory</a> containing the results of the executed tests must be provided.
                The default location is <code>./JTwork</code>.  An alternate directory may be
                specified using <code>-workDir</code>.</p>
            </answer>
        </entry>

        <entry>
            <question>How do I run <code>jtreg</code> under Windows?</question>
            <answer>
                <p><code>jtreg</code> is normally invoked by a wrapper script, written
                for the Bourne family of shells. On Windows, you can use
                <a href="http://www.mkssoftware.com/">MKS</a>,
                which uses ksh, or you can use
                <a href="http://www.cygwin.com/">Cygwin</a>, which uses ash or bash,
                depending on which version you are using.</p>
                <p>You can also start <code>jtreg</code> directly, with a command of
                the form <code>java -jar jtreg.jar <i>options</i></code>, but you
                will still need MKS or Cygwin installed to be able to run shell tests.</p>
            </answer>
        </entry>

        <entry>
            <question>Which should I use? MKS or Cygwin?</question>
            <answer>
                <p><code>jtreg</code> supports both, equally. However, the tests in the
                JDK regression test suite assume that MKS is available. So, when you
                are writing tests for that test suite, you should make sure that your
                test at least works with MKS. If you prefer to use Cygwin, and can make
                your test run with both, so much the better.</p>
                <p><strong>Note:</strong> as of JDK 8, the
                tests assume the use of Cygwin by default.</p>
            </answer>
        </entry>

        <entry>
            <question>How do I run only tests which
            were written for a specific bugid?</question>
            <answer>
                <p>The <code>-bug</code> option to <code>jtreg</code> will run only those
                tests which define the given bugid using the <code>@bug</code> tag.</p>
            </answer>
        </entry>

        <entry>
            <question>How do I run only tests NOT
            requiring manual intervention?</question>
            <answer>
                <p>The <code>-automatic</code> option to <code>jtreg</code> will ignore all
                tests which contain the <code>/manual</code> tag option.</p>
            </answer>
        </entry>

        <entry>
            <question>How do I run only tests requiring
            manual intervention?</question>
            <answer>
                <p>The <code>-manual</code> option to <code>jtreg</code> will run only those
                tests which contain the <code>/manual</code> tag option.</p>
            </answer>
        </entry>

        <entry>
            <question>How do I specify VM options to the test's JVM?
            </question>
            <answer>
                <p>Many JVM options are recognized by <code>jtreg</code> and passed
                to the test.  Note that these options are sensitive to both the platform and
                the JDK being tested.  For example, Linux does not currently support 64-bit
                operation.  Thus, the option <code>-d64</code> is not valid on
                Linux and will be rejected.</p>
                <p>You can also use the <code>-vmoption</code> or <code>-vmoptions</code>
                option to pass one or a space-separated list of options to the JVM used
                to run the test. </p>
            </answer>
        </entry>
    </group>

    <group title="TestNG tests">
        <entry>
            <question>What is the test root directory?
            </question>
            <answer>
                <p>The "test root directory", or "test suite root directory" is the
                root directory of the overall test suite.
                In OpenJDK terms, this means either of the "jdk/test/" or
                "langtools/test/" directories in an OpenJDK forest.</p>
                <p>The "test root directory" for any test is determined by finding
                the smallest enclosing directory containing a marker file called
                TEST.ROOT.
                The TEST.ROOT file can also be used to define some global properties
                for the test suite.</p>
            </answer>
        </entry>

        <entry>
            <question>Why is the "test root directory" important?
            </question>
            <answer>
                <p>Within the test suite, tests are uniquely identified by their path
                relative to the test root directory.
                This relative path is used to generate test-specific directories
                and files within the work directory, and to identify test results
                within the report directory.</p>
                <p>However, note that tests can be specified on the command line
                by any valid file system path, either absolute or relative to the
                current directory.
                </p>
            </answer>
        </entry>

        <entry>
            <question>Can I have more than one TEST.ROOT?
            </question>
            <answer>
                <p>In general, no.  Each test is uniquely associated with
                exactly one test root directory, which is the the smallest
                enclosing directory containing a marker file called TEST.ROOT.
                In general, a test run will consist of tests from a single
                test suite, identified by a single test root directory.
                </p>
                <p>It <em>is</em> possible to run tests from multiple test suites
                (such as jdk/test and langtools/test) but this is not common. </p>
            </answer>
        </entry>

        <entry>
            <question>What is a "package root"?
            </question>
            <answer>
                <p>"package root" refers the root of the package hierarchy in
                 which a Java source is placed. It is the directory you would
                 put on the javac source path if you wanted javac to compile
                 the file explicitly.</p>
                <p>Most jtreg tests are written in the "unnamed package"
                 (i.e. without a package statement) and so for most jtreg tests,
                 the directory directly containing the test is the package root
                 for the test.  This is different from other test suites using
                 other test harnesses (such as TestNG) in which all the tests
                 of a test suite are placed in a single package hierarchy,
                 often mirroring the package hierarchy of API being tested.</p>
            </answer>
        </entry>

        <entry>
            <question>How does jtreg support TestNG tests?
            </question>
            <answer>
                <p>jtreg supports TestNG tests in two ways.</p>
                <ol>
                    <li><p>Tests can be written with a full test description
                     (the comment beginning /* @test....*/) and can use the following
                     action tag to run the test with TestNG: </p>
                            <pre>"@run testng classname args"</pre>
                            <p>Such a test would otherwise be similar to a standard test
                     using "@run main". You can use other tags such as @library and
                     @build, just as you would for "@run main".
                     These tests would normally be in the unnamed directory
                     (i.e. no package statement.)
                     These tests can be freely intermixed with other tests using
                     "@run main", "@run shell", "@run applet" and so on. </p> </li>
                    <li><p>If you have a group of TestNG tests written in their own
                     package hierarchy, you can include that entire package
                     hierarchy as a subdirectory under the main test root directory.
                     If you do this, you must identify the root directory of that
                     package hierarchy to jtreg, so that it knows to treat all the
                     files in that package hierarchy specially.
                     In such a group of tests, you do not need to provide test
                     descriptions in each test.
                     You may optionally provide a test description if you choose to
                     do so, if you wish to specify information tags such as
                     @bug, @summary and @keyword.
                     You must not specify any action tags, such as @run, @compile,
                     etc, since the actions are implicit for every test in the group
                     of tests. </p></li>
                </ol>
            </answer>
        </entry>

        <entry>
            <question>How do I identify a group of TestNG tests in their own directory?
            </question>
            <answer>
                <p>Add a line specifying the directory to TEST.ROOT</p>
                <pre>TestNG.dirs = dir1 dir2 dir3 ...</pre>
                <p>Include the package root directory for each group of TestNG
                tests, and specify the package root directory relative to the test
                root directory.
                </p>
                <p>You can also override the value in TEST.ROOT by using the
                TEST.properties file in any subdirectory of the test root directory
                that contains the package root.
                If you put the declaration in a TEST.properties file, you can
                specify the path relative to the directory containing the
                TEST.properties file.
                In particular, instead of declaring the directory in TEST.ROOT,
                you could place the declaration in a TEST.properties file in the
                package root directory for the group of tests, in which case
                the declaration would be simply:
                </p>
                <pre>TestNG.dirs = .</pre>
            </answer>
        </entry>

        <entry>
            <question> How does jtreg run TestNG tests?
            </question>
            <answer>
                <p>Tests using "@run testng" are compiled in the standard way,
                with TestNG libraries on the classpath.
                The test is run using the class <code>org.testng.TestNG</code>.
                For any TestNG test in a group of TestNG tests, any tests in the
                group that need compiling are compiled together before any
                test in the group is run.
                Then, the selected test classes are run one at a time using
                <code>org.testng.TestNG</code>.
                Each test that is run will have its results stored in a
                corresponding *.jtr file.
                </p>
            </answer>
        </entry>

        <entry>
            <question>How do I specify any libraries I want to use in TestNG tests?
            </question>
            <answer>
                <p>Tests using "@run testng" can use @library and @build in
                the standard way.
                For any test in a group of TestNG tests, you can specify the
                library by adding a line specifying the library in the
                TEST.properties file in the package root directory for the group of
                tests.
                </p>
                <pre>lib.dirs = path-to-library ...</pre>
                <p>As with the @library tag, if the path begins with "/",
                it will be evaluated relative to the test root directory;
                otherwise, it will be evaluated relative to the directory
                containing the TEST.properties file.
                </p>
                <p>For any particular group of TestNG tests, you can only
                specify libraries for the entire group: you cannot specify one
                library for some of the tests and another library for other tests.
                This is because the all the source files in the group are
                compiled together.
                </p>
            </answer>
        </entry>

        <entry>
            <question>What version of TestNG does jtreg support?</question>
            <answer>
                <p>Run the command <code>jtreg -version</code> to see the
                version of jtreg and available components.</p>
            </answer>
        </entry>
    </group>

    <group title="General Problems">

        <entry>
            <question>My test only passes if I don't use jtreg to run it.
            Why does it fail in jtreg?</question>
            <answer>
                <p>By default, tests run using <code>jtreg</code> are each run in a
                separate JVM.  By design, only a limited number of shell
                environment variables are passed down to the test's JVM.
                This may affect how a test is run. </p>

                <p>As per spec, the only shell environment variables that are
                automatically propagated into the test's JVM are:</p>

                <ul>
                    <li>Linux and Solaris:
                    <ul>
                        <li><code>PATH</code> is set to <code>/bin:/usr/bin</code></li>
                        <li>The following are propogated from the user's environment:
                            <code>DISPLAY</code>,
                            <code>HOME</code>
                            <code>LANG</code>,
                            <code>LC_ALL</code>,
                            <code>LC_CTYPE</code>,
                            <code>LPDEST</code>,
                            <code>PRINTER</code>,
                            <code>TZ</code> and
                            <code>XMODIFIERS</code>
                        </li>
                    </ul></li>

                    <li>Windows:
                    <ul>
                        <li><code>PATH</code> is set to the MKS or Cygwin toolkit binary directory</li>
                        <li>The following are propogated from the user's environment:
                            <code>SystemDrive</code>,
                            <code>SystemRoot</code>
                            <code>windir</code>
                        </li>
                    </ul></li>
                </ul>

                <p>If your test needs to provide more environment variables or
                to override any values, use the <code>-e</code> option.</p>
            </answer>
        </entry>

        <entry>
            <question>How do I set the
            <code>CLASSPATH</code> environment variable for my test?</question>
            <answer>
                <p>The harness sets the <code>CLASSPATH</code> for the <code>compile</code>,
                <code>main</code>, and <code>applet</code> actions to be the system class path
                plus the test's source and classes directory.</p>

                <p>It is possible to set the classpath for the <code>main/othervm</code> action
                via the <code>-classpath</code> option to <code>java</code>.  Any other
                modification of the <code>CLASSPATH</code> must be done using the
                <code>shell</code> action.</p>
            </answer>
        </entry>

        <entry>
            <question>Why don't you just pass all of my
            shell environment variables to the JVM running the test?</question>
            <answer>
                <p>The approach of passing down a list of pre-defined environment variables
                helps guarantee consistent results across different people running the test(s).</p>
            </answer>
        </entry>

        <entry>
            <question>Why is the default to run tests in
            another JVM?</question>
            <answer>
                <p>Insulation from other tests.  Most well-behaved tests do not modify the
                JVM; however, if a test does modify the JVM it is possible that this change
                will interfere with subsequent tests.  Running each test in another JVM allows
                for the possibility of bad test suite citizens.</p>
            </answer>
        </entry>

        <entry>
            <question>Why would I ever want to run in the
            same JVM?</question>
            <answer>
                <p>Speed.</p>
            </answer>
        </entry>

        <entry>
            <question>What is "agent VM" mode, and why would I want to use it?</question>
            <answer>
                <p>It's like "same VM" mode, but better. By default, tests will run in
                the same JVM. In between tests, jtreg will try to reset the JVM to
                a standard state, and if it cannot, it will discard the JVM and
                start another. </p>
            </answer>
        </entry>

        <entry>
            <question>Should a test call the
            <code>System.exit</code> method?</question>
            <answer>
                <p><i>NO!</i> The default harness security manager prevents tests from
                calling <code>System.exit</code>.  If the test is running in the same JVM as
                the harness and the harness was not permitted to install its own security manager, a
                call to <code>System.exit</code> will cause the harness itself to exit!</p>

                <p>If the test is running in its own separate JVM, a call to
                <code>System.exit</code> may not allow the harness to properly handle test
                termination.</p>
            </answer>
        </entry>

        <entry>
            <question>My test only applies to one
            platform and it will fail/not run in others.  How do I prevent the harness from
            running it on the wrong platform?</question>
            <answer>
                <p>The <a href="tag-spec.txt">tag specification</a> provides no way to indicate
                any platform requirements.  If
                the test only applies to a single platform, then the test itself must determine
                the current platform and decide whether the test should be run there.  If the
                test suite is running on the wrong platform, the test should pass (i.e. just
                return) otherwise, the test should proceed. A significant benefit to this
                approach is that the same number of tests in a testsuite will always be run if
                the same arguments are given to <code>jtreg</code> regardless of the particular
                platform.</p>

                <p>For tests that are written in Java code (i.e. <code>applet</code> and
                <code>main</code> tests), you may determine the platform via the system
                properties.  The following code fragment may be used to distinguish between
                SunOS sparc, SunOS x86, Windows, etc.</p>

                <dl><dt></dt><dd>
                <pre>
                    String name = System.getProperty("os.name");
                    if (name.equals("Linux")) {
                        System.out.println("Linux");
                    } else if (name.contains("OS X")) {
                        System.out.println("(Mac) OS X");
                    } else if (name.equals("SunOS")) {
                        System.out.println("Solaris");
                    } else if (name.startsWith("Windows")) {
                        System.out.println("Windows");
                    } else {
                        throw new RuntimeException("unrecognized OS:" +
                                " os.name == " + name);
                    }
                </pre></dd></dl>

                <p>This approach is not suitable for <code>shell</code> tests.  In this case,
                you can determine the platform via <code>uname</code>.  The following code
                accomplishes the same task as above.</p>

                <dl><dt></dt><dd>
                <pre>
                    OS=`uname -s`
                    case "$OS" in
                        CYGWIN* )
                            echo "Windows (Cygwin)" ;;
                        Darwin )
                            echo "(Mac) OS X" ;;
                        Linux )
                            echo "Linux" ;;
                        SunOS )
                            echo "Solaris" ;;
                        Windows* )
                            echo "Windows" ;;
                        * )
                            echo "unrecognized system: $OS" ; exit 1 ;;
                    esac
                </pre></dd></dl>
            </answer>
        </entry>

        <entry>
            <question>How can I make
            <code>applet</code> and <code>main</code> action tests read from data
            files?</question>
            <answer>
                <p>When jtreg is executed, it <code>cd's</code> into a scratch area to
                ensure that a test can not alter the test suite.  Thus, a direct reference to a
                data file without some sort of indicator as to where the test was originally
                located will fail.</p>

                <p>The system property <code>test.src</code> contains the name of the directory
                where the test source resides.  The following example illustrates how to read
                the data file <code>foo</code> which is contained in the test source directory.
                Note that the default value of <code>"."</code> allows this test to run both
                with the harness, and without (in the source directory).</p>

                <dl><dt></dt><dd>
                <pre>
                    File f = new File(System.getProperty("test.src", "."), "foo");
                    InputStream in = new File(f);
                </pre></dd></dl>
            </answer>
        </entry>

        <entry>
            <question>Can I use <code>package</code>
            statements in my tests?</question>
            <answer>
                <p>Yes&#8230; but you probably don't want to.  The harness  searches for class
                files in a class directory with components which are parallel to the source
                directory.  It will be unable to locate packaged class files when the test is
                invoked via reflection.  Use of the <code>package</code> statement is not
                recommended unless the test is intended to test <code>package</code> statements.</p>

                <p>Tests which test the package mechanism may use package statements; however,
                it will be the responsibility of the test writer to properly execute the
                compiled class as necessary.</p>
            </answer>
        </entry>

        <entry>
            <question>Why can't multiple test source
            files in the same directory have package-private classes of the same
            name?</question>
            <answer>
                <p>In the Java language, package private classes defined in different files
                in the same directory are interpreted as duplicate class definitions.  The
                contents of the class files depends on the order of compilation.  To avoid
                compilation order dependency problems, we recommend that you define auxiliary
                classes as inner classes.</p>

                <p>For performance reasons, the harness does not automatically remove class files
                between individual tests or build each test within its own unique subdirectory.
                This allows us to cache class files across test runs and share code between tests
                in the same directory.</p>
            </answer>
        </entry>

        <entry>
            <question>Should a test catch
            <code>Throwable</code>, <code>Exception</code>, or
            <code>Error</code>?</question>
            <answer>
                <p>Ideally, only specific, anticipated exceptions should be caught by a
                test.  Any other exception which is provoked during testing should be
                propagated back to the harness.</p>

                <p>In the event that a very general exception is caught in test code, a certain
                amount of care will need to be exercised.  For example if a user wants to stop
                the harness during a test run, an <code>InterruptedException</code> is used to
                interrupt the test.  If that exception is caught by a test and not re-thrown to
                the harness, the stop request will be lost and the tests will not stop!</p>

                <p>Here is a list of exceptions that may need to be re-thrown:</p>
                <ul>
                    <li><code>InterruptedException</code> (from <code>Exception</code>)</li>
                    <li><code>InterruptedIOException</code> (from <code>IOException</code>)</li>
                    <li><code>ThreadDeath</code> (from <code>Error</code>)</li>
                </ul>
            </answer>
        </entry>

        <entry>
            <question>My test requires that I use
            information printed to <code>System.out</code> or <code>System.err</code> to
            determine whether a test passed or failed.  When I run my test in the harness, I
            can't seem to find these output streams.</question>
            <answer>
                <p> Currently, information sent to <code>System.out</code> or
                <code>System.err</code> is only available <em>after</em> the test has finished
                running.</p>

                <p>Note that this question indicates that the test itself can not determine
                whether it passed or failed (i.e. it needs human intervention).  Thus, the test
                uses the <code>manual</code> option.  The suggestions provided for the <a
                href="#group6"><code>applet</code> action</a> may apply.</p>
            </answer>
        </entry>

        <entry>
            <question>My test does tricky things that are
            not supported by <code>jtreg</code>. Can I still write a regression
            test?</question>
            <answer>
                <p>Yes.  Most tests can be written using a series of <code>main</code>,
                <code>clean</code>, <code>build</code>, <code>applet</code>, and
                <code>compile</code> actions.  However, there have been a few tests that need
                to do things like run a specific application or access specific environment
                variables.  The <code>shell</code> action allows a user to invoke a Bourne
                shell-script which can run arbitrary commands, including running
                <code>java</code> and <code>javac</code>.</p>

                <p><b>Warning!</b> All tests, including shell-script tests, may be run on
                multiple platforms including Linux, Solaris, Windows and Mac OS X. The
                shell-script should be written to with this in mind.  The following code
                fragment may be useful in defining various platform-dependent variables.</p>

                <dl><dt></dt><dd>
                    <pre>
                        OS=`uname -s`
                        case "$OS" in
                            SunOS | Linux | *BSD | Darwin )
                                NULL=/dev/null
                                PATHSEP=":"
                                FILESEP="/"
                                TMP=/tmp
                                ;;
                            CYGWIN* )
                                NULL=/dev/null
                                PATHSEP=";"
                                FILESEP="/"
                                TMP=/tmp
                                ;;
                            Windows* )
                                NULL=NUL
                                PATHSEP=";"
                                FILESEP="\\"
                                TMP=$TEMP
                                ;;
                            * )
                                echo "Unrecognized system!"
                                exit 1;
                                ;;
                        esac
                    </pre>
                </dd></dl>

                <p>If the <code>shell</code> action still does not provide the flexibility
                needed to write the regression test, then use the <code>ignore</code> action.
                It is also advisable to include a comment with sufficient detail to allow a
                person to run the test and verify its behavior.</p>
            </answer>
        </entry>

        <entry>
            <question>What happens if my test returns
            when there are still threads running?</question>
            <answer>
                <p>The harness runs the <code>main</code> action's <code>main</code> method in
                its own thread group.  The thread group will be destroyed by the harness when the
                <code>main</code> method returns.  It is the responsibility of the test to
                return only after the appropriate tasks have been completed by its subsidiary
                threads.</p>
            </answer>
        </entry>

        <entry>
            <question>If my bug hasn't been fixed, and
            the test is run, the JVM crashes.  How do I make sure that the test doesn't
            cause the harness to crash?</question>
            <answer>
                <p>If the symptom of a bug is a JVM crash, then the test's description
                should include the <code>othervm</code> option. This will allow the harness to
                continue running any subsequent tests, write its reports, and exit normally.</p>
            </answer>
        </entry>

        <entry>
            <question>The JavaTest harness is running into problems
            running the test because of issues with the JDK I'm trying to test. What can I
            do?</question>
            <answer>
                <p>When the harness is used to run tests, two possibly different versions of
                the JDK are used: the JDK version used to run the harness and the JDK version used
                to run the test(s) themselves.</p>

                <p>To run the harness with one version of the JDK and the tests with another, use
                the <code>-othervm</code> option in conjunction with the <code>-testjdk</code>
                option.  The <code>-testjdk</code> option will specify the version of the JDK
                to run the tests.  The environment variables <code>JT_JAVA</code> or
                <code>JAVA_HOME</code> will specify the version of the JDK for the harness.</p>
            </answer>
        </entry>

        <entry>
            <question>My test requires that I install my
            own security manager, but it appears that the JavaTest harness has already installed
            one. What do I do?</question>
            <answer>
                <p>The harness normally installs its own rather permissive security manager in
                self-defense to prevent tests from interfering with each other.  The harness'
                security manager is designed to prevent changes to the JVM's state that would
                impact other tests. Most tests will not find the standard harness security
                manager a hindrance.</p>

                <p>A test which must install its own security manager will always need to run
                in its own separate JVM. To do this, add the <code>othervm</code> option to the
                <code>main</code> action in the test description.</p>
            </answer>
        </entry>

        <entry>
            <question>Can I automate running regtests or
            can I run the tests on a regular basis?</question>
            <answer>
                <p>Yes.  If you are using a UNIX system, <code>man crontab</code> is your
                friend.  Other platforms have similar facilities (often third party) for
                automated command scheduling.</p>
            </answer>
        </entry>

        <entry>
            <question>I run all (or a huge part) of the
            regression test suite as part of a cron job or other nightly process.  I'd like
            to generate my own reports or I'd like to send myself e-mail whenever a test
            fails.  Do I have to parse the verbose output or the <code>.jtr</code>
            file?</question>
            <answer>
                <p>No. The harness supports an observer interface.  APIs exist to query test
                results at the conclusion of the test's execution.  A user can write their own
                observer to record and act on information as desired. </p>
            </answer>
        </entry>

    </group>
    <group title="Tag Problems">

        <entry>
            <question>How do I decide whether my test
            should use the <code>compile</code> action or the <code>build</code>
            action?</question>
            <answer>
                <p>The <code>build</code> action will compile the specified class only if
                the classfile doesn't exist or is older than its source.  The
                <code>compile</code> action <em>always</em> invokes the compiler.  Typically,
                the <code>compile</code> action is used only to test the compiler, while the
                <code>build</code> action is used for tests that make use of multiple sources
                or for API tests.</p>
            </answer>
        </entry>

        <entry>
            <question>When do I need to specify the
            <code>build</code> action?</question>
            <answer>
                <p>Each <code>main</code> and <code>applet</code> action contains an
                implied <code>build</code> action.  The harness will build the class specified by
                the <code>main</code> or <code>applet</code> actions as needed without any
                prompting.  If the test requires additional class(es), every additional class
                must be associated with an explicit <code>build</code> action.</p>
            </answer>
        </entry>

        <entry>
            <question>How do I decide whether my applet
            test should use the <code>main</code> action or the <code>applet</code>
            action?</question>
            <answer>
                <p>Ultimately, that decision is left to the person writing the test;
                however, the following should be considered.</p>

                <p>Tests which use the <code>applet</code> action are <i>not</i>
                necessarily restricted to tests which must run in a browser. Any
                Swing/AWT code which can be written such that it derives from
                <code>java.applet.Applet</code> or <code>javax.swing.JApplet</code> is
                a potential applet test.</p>

                <p>For tests which test graphics functionality, there are three major
                advantages to selecting the <code>applet</code> action over the
                <code>main</code> action: expanded <code>manual</code> support leading to less
                duplicated code per test, thread synchronization, and cleanup.</p>

                <p>Frequently, tests which test graphics functionality need some sort of user
                interaction to determine whether the test behaves as expected. The
                <code>applet</code> action takes care of providing a user interface which
                contains instructions for the user and the appropriate interface to indicate
                <code>pass</code>, <code>fail</code>, or <code>done</code> as indicated by the
                <code>manual</code> option.  User instructions are taken from the
                <code>.html</code> file referenced in the <code>applet</code> action.  Each
                <code>main</code> action which tests graphics functionality must implement
                their own version of this interface.  This path leads to more code needed per
                test and less consistency across tests in the test suite.</p>

                <p>A <code>main</code> action test is deemed to be completed when the
                <code>main</code> method returns. A test which requires multiple threads must
                take care not to allow the main method to return before those other threads
                have completed. The <code>applet</code> action handles basic AWT thread
                synchronization.</p>

                <p>Finally, the <code>applet</code> action handles test cleanup.  If a test can
                not or does not dispose top-level windows or any AWT threads, they will be
                eliminated by the harness after the test completes.</p>
            </answer>
        </entry>

        <entry>
            <question>I put in an <code>ignore</code> tag
            into my test description but my test wasn't ignored.</question>
            <answer>
                <p> The <code>ignore</code> tag should be used for tests that are too
                complex for the currently defined set of tags or for tests that should be
                temporarily ignored.  The <code>ignore</code> tag instructs the harness to ignore
                that and any <i>subsequent</i> tags.  Check the location of the
                <code>ignore</code> tag.</p>
            </answer>
        </entry>

        <entry>
            <question>Can I use the <code>@author</code>,
            <code>@run</code>, etc. tags in other files?</question>
            <answer>
                <p>Yes. The tags may be used for documentation purposes in any file. Only
                those comments whose leading tag is <code>@test</code> is considered a test
                description.</p>
            </answer>
        </entry>

    </group>
    <group title="Applet Problems">

        <entry>
            <question>My <code>/manual</code> test sends
            events to <code>System.out/System.err</code> so that the user can determine
            whether the test behaved properly.  How do I write my test if I can't see these
            output streams?</question>
            <answer>
                <p>The test code should be written to determine whether a test has passed or
                failed based on events generated during a given time-frame. Use the
                <code>/manual=done</code> option of the <code>applet</code> action to set the
                time frame.  If the user has not generated the expected event before the
                <code>done</code> button has been pressed, the test should detect this in the
                <code>destroy</code> method and throw an exception.</p>

                <p>While this approach takes potentially more time to implement, it avoids user
                error which may occur in checking the event.  This scheme also avoids string
                comparison of events. (A much safer way to determine whether the expected event
                has been received is to check the event type, coordinates, modifiers, etc.)</p>

                <p><b>Warning!</b> The AWT event thread does not propagate exceptions!  It is
                recommended that all exceptions indicating failure of the test be thrown from
                one of the methods called by the harness.  (i.e. <code>init()</code>,
                <code>start()</code>, <code>stop()</code>, <code>destroy()</code>)</p>

                <p>The following simple <code>applet</code> test illustrates the recommended
                behavior.</p>

                <p>Basic <code>.html</code> test description file.</p>

                <dl><dt></dt><dd>
                    <pre>
                        &lt;html&gt;
                            &lt;body&gt;

                                &lt;!--
                                    @test
                                    @bug 2997924
                                    @summary Sample test that verifies an event
                                    @run applet/manual=done SampleTest.html
                                --&gt;

                                &lt;applet code=SampleTest width=200 height=50&gt;&lt;/applet&gt;

                                Select the "pick me" check box.

                            &lt;/body&gt;
                        &lt;/html&gt;
                    </pre>
                </dd></dl>

                <p>The sample test code.</p>

                <dl><dt></dt><dd>
                    <pre>
                        import java.applet.Applet;
                        import java.awt.Checkbox;
                        import java.awt.FlowLayout;
                        import java.awt.Panel;
                        import java.awt.event.ItemEvent;
                        import java.awt.event.ItemListener;

                        // WARNING! The AWT event thread does not propagate exceptions!
                        // It is recommended that all exceptions indicating failure
                        // of the test be thrown from one of the methods called by the harness.
                        // (i.e. init(), start(), stop(), destroy())

                        public class SampleTest extends Applet {
                            public void init() {
                                setLayout(new FlowLayout());
                                add(new TestPanel(this));
                            }

                            public void destroy() {
                                if (myEvent == null)
                                    throw new RuntimeException("no events");
                                else {
                                    Checkbox cb = (Checkbox)(myEvent.getItemSelectable());
                                    if (!(cb.getLabel().equals("pick me!") &amp;&amp; cb.getState()))
                                        throw new RuntimeException("unexpected last event");
                                }
                            }

                            class TestPanel extends Panel {
                                Checkbox pickMe, notMe;
                                Listener listener = new Listener();
                                Applet applet;

                                public TestPanel(Applet myApplet) {
                                    applet = myApplet;
                                    pickMe = new Checkbox("pick me!");
                                    notMe  = new Checkbox("not me");

                                    pickMe.addItemListener(listener);
                                    notMe.addItemListener(listener);

                                    add(pickMe);
                                    add(notMe);
                                }

                                class Listener implements ItemListener {
                                    // Don't throw an exception here.  The awt event thread
                                    // will NOT propagate your exception!
                                    public void itemStateChanged(ItemEvent event) {
                                        System.out.println("event: " + event);
                                        myEvent = event;
                                    }
                                }
                            }

                            private ItemEvent myEvent;
                        }
                    </pre>
                </dd></dl>
            </answer>
        </entry>

        <entry>
            <question>I threw an exception, the output
            was sent to <code>System.err</code>, but my test still passed.  What
            happened?</question>
            <answer>
                <p>Verify that the exception was not thrown by the event thread.  The event
                thread does not propagate exceptions.  Furthermore, the event thread is in a
                separate thread group and the harness cannot catch exceptions thrown from there.
                It is <em>strongly</em> recommended that all exceptions indicating failure of
                the test be thrown from one of the methods called by the harness.
                (i.e. <code>init()</code>, <code>start()</code>, <code>stop()</code>,
                <code>destroy()</code>)</p>
            </answer>
        </entry>

        <entry>
            <question>My <code>applet</code> action test
            didn't run my <code>main</code> method!</question>
            <answer>
                <p><code>applet</code> action tests do not call <code>main</code>. A test
                which uses the <code>applet</code> action is run by invoking its
                <code>init</code>, <code>start</code>, and <code>setVisible(true)</code>
                methods. Depending on the value of <code>manual</code>, the harness will pause
                either for a few seconds or until the user clicks on the <code>pass</code>,
                <code>fail</code>, or <code>done</code> buttons. Finally, the harness will invoke
                the <code>stop</code> and <code>destroy</code> methods.</p>

                <p>The <code>main</code> method of an <code>applet</code> action will only be
                used if the test was run outside of the harness.</p>
            </answer>
        </entry>

        <entry>
            <question>If I have an applet test, do I put
            the test description in the <code>.html</code> file or the <code>.java</code>
            file?</question>
            <answer>
                <p>It doesn't matter. When <code>jtreg</code> is run on a test suite or
                directory, the test description will be found regardless of the file
                particulars.  When running a single test, <code>jtreg</code> must be invoked on
                the file which contains the test description.</p>

            </answer>
        </entry>

        <entry>
            <question>For my <code>/manual</code> tests,
            how do I provide the user instructions to run the test?</question>
            <answer>
                <p>User instructions should be provided in the applet's HTML file.  The
                uninterpreted HTML file will be displayed by the <code>applet</code> action in
                a TextArea labelled <code>html file instructions:</code>.</p>
            </answer>
        </entry>

        <entry>
            <question>For <code>/manual</code> tests, how
            is the initial size of the running applet determined?</question>
            <answer>
                <p>The size of the applet is statically determined by the
                <code>height</code> and <code>width</code> attributes provided to the HTML
                <code>applet</code> tag.  The applet interface provides a way to dynamically
                change the size of the applet by setting the <code>applet size:</code> to
                "<code>variable</code>".</p>
            </answer>
        </entry>
    </group>
    <group title="Deciphering Common Harness Errors">

        <entry>
            <question><code>Failed. Unexpected exit from test</code></question>
            <answer>
                <p><b>Answer:</b> The test has completed in an unexpected manner.
                This could be caused by some sort of fault (e.g. a segmentation fault)
                or because the harness has detected a call to <code>System.exit</code>
                from the test.</p>

                <p>Tests are not allowed to call <code>System.exit</code> because the
                test must have the ability to run in the same JVM as the harness.
                Calling <code>System.exit</code> while the test is running in this
                manner whould cause the harness itself to exit!  Instead of calling
                <code>System.exit()</code>, throw an exception.</p>

                <p>Be warned that the AWT event thread does not propagate exceptions,
                so if the test was exiting from the event thread, it is not sufficient
                to simply throw an exception.  The test must set some variable which
                can be used to throw an exception from one of the methods called by
                the harness. (i.e. <code>init()</code>, <code>start()</code>,
                <code>stop()</code>, or <code>destroy()</code>)</p>
            </answer>
        </entry>

        <entry>
            <question><code>Error. Can't find `main' method</code></question>
            <answer>
                <p><b>More symptoms</b>: In <code>System.err</code>, you get a stack trace
                for an <code>java.lang.NoSuchMethodException</code> and some harness messages.</p>

                <dl><dt></dt><dd>
                    <pre>
                        java.lang.NoSuchMethodException
                         at java.lang.Class.getMethod(Class.java)
                         at com.sun.javatest.regtest.MainWrapper$MainThread.run(MainWrapper.java:89)
                         at java.lang.Thread.run(Thread.java)

                        JavaTest Message: main() method must be in a public class named
                        JavaTest Message: ClassNotPublic in file ClassNotPublic.java
                    </pre>
                </dd></dl>

                <p><b>Answer</b>: The class defining the test must be declared
                <code>public</code> and the class name must be the basename of the
                <code>.java</code> file; otherwise the harness will not be able to use reflection
                to invoke the test.</p>

            </answer>
        </entry>

        <entry>
            <question><code>Error. Parse Exception: No
            class provided for `main'</code></question>
            <answer>
                <p><b>Answer</b>: An <code>@run main</code> tag was provided
                without the required class name.  Either provide the name of the class
                or remove the line entirely if appropriate.</p>

                <p>The line may be removed without impacting the test if all of the following
                criteria are met:</p>
                <ul>
                    <li>The file containing the test description has the <code>.java</code>
                    extension.</li>
                    <li>This is the only <code>@run</code> tag in the test description.</li>
                    <li>No options to <code>main</code> are required</li>
                </ul>
                <p>In removing the line, we take advantage of the default action for
                <code>.java</code> files.</p>
            </answer>
        </entry>

        <entry>
            <question><code>Error. Parse Exception:
            `applet' requires exactly one file argument</code></question>
            <answer>
                <p><b>Answer</b>: The applet action requires a single argument which should
                be the name of the <code>.html</code> file which contains (at minimum) the HTML
                <code>applet</code> tag.</p>
            </answer>
        </entry>

        <entry>
            <question><code>Error. Parse Exception:
            `archive' not supported in file: &#8230; </code></question>
            <answer>
                <p><b>More Symptoms</b>: The test is an <code>applet</code> action test.
                The HTML <code>applet</code> tag includes the <code>archive</code> attribute.</p>

                <p><b>Answer</b>: The regression extensions to the harness do not support the
                <code>archive</code> attribute.</p>
            </answer>
        </entry>

        <entry>
            <question><code>test results:no tests
            selected</code></question>
            <answer>
                <p><b>More Symptoms</b>: At a terminal window, you get:</p>

                <dl><dt></dt><dd>
                    <pre>
                        test results:no tests selected
                        Report written to /home/iag/work/doc/JTreport/report.html
                        Results written to /home/iag/work/doc/JTwork
                    </pre>
                </dd></dl>

                <p><b>Answer</b>: No test descriptions were found by
                <code>jtreg</code> in the file or directory specified.  If the
                <code>-automatic</code> option to <code>jtreg</code> was provided,
                then there were no tests which did not include the
                <code>/manual</code> tag option.</p>

                <p>Verify that the first tag of each test description is
                <code>@test</code>.</p>
            </answer>
        </entry>

        <entry>
            <question><code>Test does not have unique name
            within work directory</code></question>
            <answer>
                <p><b>More Symptoms</b>: At a terminal window, you get:</p>

                <dl><dt></dt><dd>
                    <pre>
                        Error:
                         -- Test does not have unique name within work directory
                         -- Test name: Duplicate
                         -- Used by test: Duplicate.java
                         -- And by test:  Duplicate.html
                         -- Overwriting results of first test
                    </pre>
                </dd></dl>

                <p>A single directory contains more than one file with the same basename.</p>

                <p><b>Answer</b>: The two files contain descriptions of tests and the harness is
                unable to determine a unique <a href="#question3.5"><code>.jtr</code></a> filename so
                the harness will overwrite the results of the first test.  It is possible to have
                a single directory with more than one file with the same basename; however,
                only one of those files may have a test description (<code>@test</code> is the
                first token of the comment).</p>

                <p>If both files contain identical test descriptions, select one file to
                contain the primary test description. Disable the other test description by
                either removal of the entire comment or simply the <code>@test</code> tag.</p>

                <p>If the files contain unique test descriptions, one of the basefile names must
                be changed.</p>
            </answer>
        </entry>

        <entry>
            <question><code>Error. JUnit not available</code></question>
            <answer>
                <p>To run JUnit tests within jtreg, you must have a copy of junit.jar
                available. To do this, you should do one of the following:</p>
                <ul>
                <li>Put junit.jar on the classpath used to run jtreg.</li>
                <li>You can specify the location by setting the system property
                        <code>junit.jar</code></li>
                <li>Install a copy in the <em>jtreg</em><code>/lib</code> directory
                        if it is not already present.</li>
                </ul>
                <p>
                If you do not have a copy of junit.jar on your system, you can download
                it from <a href="http://junit.org/">http://junit.org/</a>.
                </p>
            </answer>
        </entry>
    </group>
</faq>
